{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-04T00:10:55.393887Z",
     "iopub.status.busy": "2025-09-04T00:10:55.393404Z",
     "iopub.status.idle": "2025-09-04T00:12:09.604374Z",
     "shell.execute_reply": "2025-09-04T00:12:09.603680Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nbtest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load the dataset and prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-04T00:12:09.607956Z",
     "iopub.status.busy": "2025-09-04T00:12:09.607312Z",
     "iopub.status.idle": "2025-09-04T00:12:11.041193Z",
     "shell.execute_reply": "2025-09-04T00:12:11.040767Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./input/train.csv\")\n",
    "\n",
    "# Drop unnecessary columns for the task\n",
    "columns_to_drop = [\"source\", \"extra_note\", \"flag\", \"id\"]\n",
    "df.drop(columns=columns_to_drop, inplace=True)\n",
    "\n",
    "# Split original DataFrame into train/test\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "'''\n",
    "TODO: Write an assertion for `train_df` to check that the unnecessary columns \n",
    "      (i.e., \"source\", \"extra_note\", \"flag\", \"id\") are indeed dropped.\n",
    "'''\n",
    "\n",
    "X_train = train_df.drop(\"label\", axis=1) / 255.0  # normalized value\n",
    "y_train = train_df[\"label\"]                     \n",
    "\n",
    "X_test = test_df.drop(\"label\", axis=1) / 255.0\n",
    "y_test = test_df[\"label\"]\n",
    "\n",
    "X_train = X_train.values.reshape(-1, 28, 28, 1)\n",
    "X_test = X_test.values.reshape(-1, 28, 28, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Define a convolutional neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-04T00:12:11.043449Z",
     "iopub.status.busy": "2025-09-04T00:12:11.043255Z",
     "iopub.status.idle": "2025-09-04T00:12:11.045711Z",
     "shell.execute_reply": "2025-09-04T00:12:11.045383Z"
    }
   },
   "outputs": [],
   "source": [
    "conv_filters = 16        \n",
    "kernel_size = (3, 3)\n",
    "dropout_rate = 0.5\n",
    "dense_units = 32         \n",
    "epochs = 2               \n",
    "batch_size = 32          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-04T00:12:11.047560Z",
     "iopub.status.busy": "2025-09-04T00:12:11.047255Z",
     "iopub.status.idle": "2025-09-04T00:12:11.121339Z",
     "shell.execute_reply": "2025-09-04T00:12:11.121018Z"
    }
   },
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Conv2D(conv_filters, kernel_size, activation='relu', input_shape=(28, 28, 1)), # Convolutional layer with ReLU activation\n",
    "    MaxPooling2D(pool_size=(2, 2)), # Max pooling layer\n",
    "    Conv2D(conv_filters, kernel_size, activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Flatten(), # Flatten the 2D output into 1D feature vector for the dense layers\n",
    "    Dense(dense_units, activation='relu'), # Fully connected layer with ReLU activation\n",
    "    Dropout(dropout_rate), # Dropout layer to reduce overfitting by randomly dropping units\n",
    "    Dense(10, activation='softmax') # Output layer with 10 units (for 10 classes) using softmax activation for classification\n",
    "])\n",
    "\n",
    "'''\n",
    "TODO: Write an assertion for `model` to check the number of layers is 8.\n",
    "'''\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-04T00:12:11.154185Z",
     "iopub.status.busy": "2025-09-04T00:12:11.154005Z",
     "iopub.status.idle": "2025-09-04T00:12:22.009622Z",
     "shell.execute_reply": "2025-09-04T00:12:22.009139Z"
    }
   },
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_split=0.1) # Train the model on the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-04T00:12:22.012023Z",
     "iopub.status.busy": "2025-09-04T00:12:22.011810Z",
     "iopub.status.idle": "2025-09-04T00:12:22.551871Z",
     "shell.execute_reply": "2025-09-04T00:12:22.551400Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "loss, accuracy = model.evaluate(X_test, y_test, verbose=1) # Evaluate the model performance\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "# TODO: Write an assertion for `accuracy` to check that the model's accuracy is within in the expected range.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
